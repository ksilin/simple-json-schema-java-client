/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package com.example;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import io.confluent.kafka.serializers.json.KafkaJsonSchemaDeserializer;
import io.confluent.kafka.serializers.json.KafkaJsonSchemaDeserializerConfig;
import io.confluent.kafka.serializers.json.KafkaJsonSchemaSerializer;
import io.confluent.kafka.serializers.json.KafkaJsonSchemaSerializerConfig;
import org.apache.kafka.clients.CommonClientConfigs;
import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;
import org.apache.kafka.common.config.SaslConfigs;

import java.io.FileInputStream;
import java.io.IOException;
import java.time.Duration;
import java.util.Collections;
import java.util.List;
import java.util.Map;
import java.util.Properties;
import java.util.concurrent.Future;

public class ProducerApp {
    public static Properties loadProperties(String fileName) throws IOException {
        final Properties props = new Properties();
        final FileInputStream input = new FileInputStream(fileName);
        props.load(input);
        input.close();
        return props;
    }


    public static void main(String[] args) throws JsonProcessingException {

        final Properties props = new Properties();

        String bootstrap = System.getenv("BOOTSTRAP");
        String kafkaKey = System.getenv("KAFKA_KEY");
        String kafkaSecret = System.getenv("KAFKA_SECRET");
        String srBasicAuth = System.getenv("SR_BASIC_AUTH");
        String schemaID = System.getenv("SCHEMA_ID");
        String topic = System.getenv("TOPIC");
        Integer limit = Integer.valueOf(System.getenv("LIMIT"));

        props.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, bootstrap);

        props.put("ssl.endpoint.identification.algorithm", "https");
        props.put(SaslConfigs.SASL_MECHANISM, "PLAIN");

        props.put(SaslConfigs.SASL_JAAS_CONFIG, "org.apache.kafka.common.security.plain.PlainLoginModule required username=\"" + kafkaKey + "\" password=\"" + kafkaSecret + "\";");
        props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SASL_SSL");

        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaJsonSchemaSerializer.class);
        props.put(KafkaJsonSchemaSerializerConfig.AUTO_REGISTER_SCHEMAS, false);
        props.put(KafkaJsonSchemaSerializerConfig.USE_SCHEMA_ID, schemaID);

        // Caused by: java.io.IOException: Incompatible schema
        // '{"$schema":"http://json-schema.org/draft-07/schema#","title":"User","type":"object","properties":{"name":{"type":"string"},"age":{"type":"integer"}},"required":["name","age"]}'
        // with refs '[]' of type 'JSON' for schema
        // '{"$schema":"http://json-schema.org/draft-07/schema#","title":"User","type":"object","additionalProperties":false,"properties":{"name":{"oneOf":[{"type":"null","title":"Not included"},{"type":"string"}]},"age":{"type":"integer"}},"required":["age"]}'. Set id.compatibility.strict=false to disable this check
        props.put(KafkaJsonSchemaSerializerConfig.ID_COMPATIBILITY_STRICT, false);

        props.put("schema.registry.url", "https://psrc-lgkvv.europe-west3.gcp.confluent.cloud");
        props.put("basic.auth.credentials.source", "USER_INFO");
        props.put("schema.registry.basic.auth.user.info", srBasicAuth);

        User user = new User("John Doe", 30);

        String jsonString = "{\"name\":\"John Doe\",\"age\":30,\"email\":\"john.doe@example.com\"}";

        ObjectMapper objectMapper = new ObjectMapper();
        JsonNode jsonNode = objectMapper.readTree(jsonString);


        try (final KafkaProducer<String, JsonNode> producer = new KafkaProducer<>(props)) {
            var producerRecord = new ProducerRecord<>(topic, "key", jsonNode);
            int produced = 0;
            while (produced < limit) {
                System.out.println("producing record nr " + produced);
                final Future<RecordMetadata> records = producer.send(producerRecord);
                producer.flush();
                produced = produced+1;
                }
            }
    }
}
